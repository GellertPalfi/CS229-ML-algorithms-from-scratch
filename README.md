# CS229 Machine Learning Algorithms
Concise implementations of fundamental machine learning algorithms from [Stanford's CS229 course](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)

# Table of content:
 - [Visualizations](#visualizations)
 - [Disclaimer](#disclaimer)
 - [Linear regression](#lin-reg)
 - [Logistic regression](#log-reg)
 - [Naive Bayes](#naive-bayes)
 - [Support Vector Machine](#svm)
 - [Simple Neural Networks](#simple-NN)
 - [Backpropagation](#back-prop)
 - [Batch Gradient Descent](#grad-desc)
 - [Useful links](#useful-links)

<a id="visualizations"></a>
# Visualizations:
Because looking at code is not the most interesting thing,  
here are some visualizations (Code included in the repo) that I made with my implementations of the algorithms.

### Linear regression line fit:
![lin_reg_fit](https://github.com/GellertPalfi/CS229/assets/69762257/a4daed4c-1753-45c6-9de3-f09c07763de1)

### Gradient descent searching for optimal parameters on the error surface in the weight space:
![grad_descent_progression](https://github.com/GellertPalfi/CS229/assets/69762257/e59efabd-494e-4515-b9bb-4dfd7c9b42e7)

<a id="disclaimer"></a>
# Disclaimer
These algorithms are very simple and primitive implementations of those found in popular ml packages such as sci-kit learn, which have been refined and optimized by experts for years, so any of the implementations here should only be used for learning purposes.

<a id="lin-reg"></a>
# Linear regression

<a id="log-reg"></a>
# Logistic regression

<a id="naive-bayes"></a>
# Naive Bayes

<a id="svm"></a>
# Support Vector Machine

<a id="simple-NN"></a>
# Simple Neural Networks

<a id="back-prop"></a>
# Backpropagation

<a id="grad-desc"></a>
# Batch Gradient Descent

<a id="useful-links"></a>
# Useful links
